{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "invisible-australian",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "continental-galaxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cathedral-washington",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/antoine/Projects/torchextractor/.env/lib/python3.6/site-packages (1.8.0)\n",
      "Requirement already satisfied: typing-extensions in /home/antoine/Projects/torchextractor/.env/lib/python3.6/site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: dataclasses in /home/antoine/Projects/torchextractor/.env/lib/python3.6/site-packages (from torch) (0.8)\n",
      "Requirement already satisfied: numpy in /home/antoine/Projects/torchextractor/.env/lib/python3.6/site-packages (from torch) (1.19.5)\n",
      "Requirement already satisfied: torchvision in /home/antoine/Projects/torchextractor/.env/lib/python3.6/site-packages (0.9.0)\n",
      "Requirement already satisfied: numpy in /home/antoine/Projects/torchextractor/.env/lib/python3.6/site-packages (from torchvision) (1.19.5)\n",
      "Requirement already satisfied: torch==1.8.0 in /home/antoine/Projects/torchextractor/.env/lib/python3.6/site-packages (from torchvision) (1.8.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/antoine/Projects/torchextractor/.env/lib/python3.6/site-packages (from torchvision) (8.1.2)\n",
      "Requirement already satisfied: typing-extensions in /home/antoine/Projects/torchextractor/.env/lib/python3.6/site-packages (from torch==1.8.0->torchvision) (3.7.4.3)\n",
      "Requirement already satisfied: dataclasses in /home/antoine/Projects/torchextractor/.env/lib/python3.6/site-packages (from torch==1.8.0->torchvision) (0.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install torchvision\n",
    "\n",
    "# Uncomment one of the following\n",
    "# !pip install torchextractor  # stable\n",
    "!pip install git+https://github.com/antoinebrl/torchextractor.git  # latest\n",
    "# import sys, os; sys.path.insert(0, os.path.abspath(\"../..\"))  # current code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "primary-grounds",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchextractor as tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accessory-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18()\n",
    "dummy_input = torch.rand(7, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-mystery",
   "metadata": {},
   "source": [
    "### List module names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fossil-knitting",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1\n",
      "layer1.0\n",
      "layer1.0.conv1\n",
      "layer1.0.bn1\n",
      "layer1.0.relu\n",
      "layer1.0.conv2\n",
      "layer1.0.bn2\n",
      "layer1.1\n",
      "layer1.1.conv1\n",
      "layer1.1.bn1\n",
      "layer1.1.relu\n",
      "layer1.1.conv2\n",
      "layer1.1.bn2\n",
      "layer2\n",
      "layer2.0\n",
      "layer2.0.conv1\n",
      "layer2.0.bn1\n",
      "layer2.0.relu\n",
      "layer2.0.conv2\n",
      "layer2.0.bn2\n",
      "layer2.0.downsample\n",
      "layer2.0.downsample.0\n",
      "layer2.0.downsample.1\n",
      "layer2.1\n",
      "layer2.1.conv1\n",
      "layer2.1.bn1\n",
      "layer2.1.relu\n",
      "layer2.1.conv2\n",
      "layer2.1.bn2\n",
      "layer3\n",
      "layer3.0\n",
      "layer3.0.conv1\n",
      "layer3.0.bn1\n",
      "layer3.0.relu\n",
      "layer3.0.conv2\n",
      "layer3.0.bn2\n",
      "layer3.0.downsample\n",
      "layer3.0.downsample.0\n",
      "layer3.0.downsample.1\n",
      "layer3.1\n",
      "layer3.1.conv1\n",
      "layer3.1.bn1\n",
      "layer3.1.relu\n",
      "layer3.1.conv2\n",
      "layer3.1.bn2\n",
      "layer4\n",
      "layer4.0\n",
      "layer4.0.conv1\n",
      "layer4.0.bn1\n",
      "layer4.0.relu\n",
      "layer4.0.conv2\n",
      "layer4.0.bn2\n",
      "layer4.0.downsample\n",
      "layer4.0.downsample.0\n",
      "layer4.0.downsample.1\n",
      "layer4.1\n",
      "layer4.1.conv1\n",
      "layer4.1.bn1\n",
      "layer4.1.relu\n",
      "layer4.1.conv2\n",
      "layer4.1.bn2\n",
      "avgpool\n",
      "fc\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-purpose",
   "metadata": {},
   "source": [
    "### Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "southeast-worship",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer1': torch.Size([7, 64, 56, 56]),\n",
       " 'layer2': torch.Size([7, 128, 28, 28]),\n",
       " 'layer3': torch.Size([7, 256, 14, 14]),\n",
       " 'layer4': torch.Size([7, 512, 7, 7])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.resnet18()\n",
    "model = tx.Extractor(model, [\"layer1\", \"layer2\", \"layer3\", \"layer4\"])\n",
    "\n",
    "model_output, features = model(dummy_input)\n",
    "{name: f.shape for name, f in features.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-taiwan",
   "metadata": {},
   "source": [
    "### Extract features from nested modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "committed-costs",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer1': torch.Size([7, 64, 56, 56]),\n",
       " 'layer2.1.conv1': torch.Size([7, 128, 28, 28]),\n",
       " 'layer3.0.downsample.0': torch.Size([7, 256, 14, 14]),\n",
       " 'layer4.0': torch.Size([7, 512, 7, 7])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.resnet18()\n",
    "model = tx.Extractor(model, [\"layer1\", \"layer2.1.conv1\", \"layer3.0.downsample.0\", \"layer4.0\"])\n",
    "\n",
    "model_output, features = model(dummy_input)\n",
    "{name: f.shape for name, f in features.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-albany",
   "metadata": {},
   "source": [
    "### Filter modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "independent-energy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv1': torch.Size([7, 64, 112, 112]),\n",
       " 'layer1.0.conv1': torch.Size([7, 64, 56, 56]),\n",
       " 'layer1.0.conv2': torch.Size([7, 64, 56, 56]),\n",
       " 'layer1.1.conv1': torch.Size([7, 64, 56, 56]),\n",
       " 'layer1.1.conv2': torch.Size([7, 64, 56, 56]),\n",
       " 'layer2.0.conv1': torch.Size([7, 128, 28, 28]),\n",
       " 'layer2.0.conv2': torch.Size([7, 128, 28, 28]),\n",
       " 'layer2.0.downsample.0': torch.Size([7, 128, 28, 28]),\n",
       " 'layer2.1.conv1': torch.Size([7, 128, 28, 28]),\n",
       " 'layer2.1.conv2': torch.Size([7, 128, 28, 28]),\n",
       " 'layer3.0.conv1': torch.Size([7, 256, 14, 14]),\n",
       " 'layer3.0.conv2': torch.Size([7, 256, 14, 14]),\n",
       " 'layer3.0.downsample.0': torch.Size([7, 256, 14, 14]),\n",
       " 'layer3.1.conv1': torch.Size([7, 256, 14, 14]),\n",
       " 'layer3.1.conv2': torch.Size([7, 256, 14, 14]),\n",
       " 'layer4.0.conv1': torch.Size([7, 512, 7, 7]),\n",
       " 'layer4.0.conv2': torch.Size([7, 512, 7, 7]),\n",
       " 'layer4.0.downsample.0': torch.Size([7, 512, 7, 7]),\n",
       " 'layer4.1.conv1': torch.Size([7, 512, 7, 7]),\n",
       " 'layer4.1.conv2': torch.Size([7, 512, 7, 7])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.resnet18()\n",
    "module_filter_fn = lambda module, name: isinstance(module, torch.nn.Conv2d)\n",
    "model = tx.Extractor(model, module_filter_fn=module_filter_fn)\n",
    "\n",
    "model_output, features = model(dummy_input)\n",
    "{name: f.shape for name, f in features.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-raising",
   "metadata": {},
   "source": [
    "### ONNX export with named output nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "amazing-demand",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18()\n",
    "model = tx.Extractor(model, [\"layer3\", \"layer4\"])\n",
    "\n",
    "torch.onnx.export(model, dummy_input, \"resnet.onnx\", output_names=[\"classifier\", \"layer3\", \"layer4\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-causing",
   "metadata": {},
   "source": [
    "### Custom Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "parliamentary-swedish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer3: 10 items\n",
      "    1 - torch.Size([7, 256, 14, 14])\n",
      "    2 - torch.Size([7, 256, 14, 14])\n",
      "    3 - torch.Size([7, 256, 14, 14])\n",
      "    4 - torch.Size([7, 256, 14, 14])\n",
      "    5 - torch.Size([7, 256, 14, 14])\n",
      "    6 - torch.Size([7, 256, 14, 14])\n",
      "    7 - torch.Size([7, 256, 14, 14])\n",
      "    8 - torch.Size([7, 256, 14, 14])\n",
      "    9 - torch.Size([7, 256, 14, 14])\n",
      "    10 - torch.Size([7, 256, 14, 14])\n",
      "layer4: 10 items\n",
      "    1 - torch.Size([7, 512, 7, 7])\n",
      "    2 - torch.Size([7, 512, 7, 7])\n",
      "    3 - torch.Size([7, 512, 7, 7])\n",
      "    4 - torch.Size([7, 512, 7, 7])\n",
      "    5 - torch.Size([7, 512, 7, 7])\n",
      "    6 - torch.Size([7, 512, 7, 7])\n",
      "    7 - torch.Size([7, 512, 7, 7])\n",
      "    8 - torch.Size([7, 512, 7, 7])\n",
      "    9 - torch.Size([7, 512, 7, 7])\n",
      "    10 - torch.Size([7, 512, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.resnet18()\n",
    "\n",
    "# Concatenate outputs of every runs\n",
    "def capture_fn(module, input, output, module_name, feature_maps):\n",
    "    if module_name not in feature_maps:\n",
    "        feature_maps[module_name] = []\n",
    "    feature_maps[module_name].append(output)\n",
    "    \n",
    "\n",
    "extractor = tx.Extractor(model, [\"layer3\", \"layer4\"], capture_fn=capture_fn)\n",
    "\n",
    "for i in range(10):\n",
    "    x = torch.rand(7, 3, 224, 224)\n",
    "    model(x)\n",
    "\n",
    "feature_maps = extractor.collect()\n",
    "for name, features in feature_maps.items():\n",
    "    print(f\"{name}: {len(features)} items\")\n",
    "    for i, f in enumerate(features):\n",
    "        print(f\"    {i+1} - {f.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
